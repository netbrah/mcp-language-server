# MCP Language Server: HTTP Streaming & Docker Optimization Analysis

## Executive Summary

This document provides research and recommendations for optimizing the mcp-language-server with HTTP streaming transport and Docker containerization, specifically addressing memory/performance concerns.

## Question 1: HTTP Streaming Transport

### Implementation Status
âœ… **Implemented** - The mcp-go library (v0.25.0) already includes SSE/HTTP transport support.

### Advantages of HTTP/SSE Transport

#### For Docker/Container Deployments
1. **Natural container networking**: HTTP works seamlessly with container networking, load balancers, and service meshes
2. **Better orchestration**: Integrates with Kubernetes, Docker Swarm, and cloud platforms
3. **Standard load balancing**: Can use nginx, HAProxy, or cloud load balancers
4. **Health checks**: Built-in health endpoints for monitoring and orchestration
5. **Horizontal scaling**: Easier to scale across multiple instances

#### For Development & Operations
1. **Debugging**: Standard HTTP tools (curl, Postman, browser dev tools)
2. **Monitoring**: Works with Prometheus, Grafana, and standard HTTP metrics
3. **Logging**: Standard access logs and request tracing
4. **Security**: Can add authentication, rate limiting, TLS via reverse proxy

#### Network Advantages
1. **Firewall-friendly**: Standard HTTP/HTTPS ports
2. **Proxy support**: Works through corporate proxies
3. **Cross-network**: Can work across different network boundaries
4. **CDN/caching**: Can leverage HTTP caching strategies

### Disadvantages of HTTP/SSE vs stdio

1. **Overhead**: HTTP headers add ~500 bytes per request
2. **Latency**: ~5-10ms additional latency for local connections
3. **Complexity**: Requires HTTP server setup and management
4. **Connection management**: Need to handle reconnections, keep-alives

### Recommendation
**Use HTTP/SSE for:**
- Container deployments
- Remote/cloud deployments
- Multi-user scenarios
- Production environments with monitoring needs

**Use stdio for:**
- Local development
- Single-user desktop applications
- Minimal latency requirements
- Simple deployment scenarios

## Question 2: Docker Container with clangd Binary

### Implementation Status
âœ… **Implemented** - Dockerfile includes clangd v19.1.5

### Advantages

1. **All-in-one solution**
   - No need to install clangd separately
   - Consistent versions across deployments
   - Simplified setup for C/C++ projects

2. **Reproducibility**
   - Same environment every time
   - No dependency conflicts
   - Easy to version and rollback

3. **Portability**
   - Works on any platform with Docker
   - No system-specific installations
   - Easy to share and deploy

4. **Resource control**
   - Can set memory limits: `--memory=2g`
   - Can limit CPU: `--cpus=2`
   - Better resource management

### Disadvantages

1. **Image size**: ~200MB (vs ~50MB without clangd)
2. **Update complexity**: Need to rebuild image for clangd updates
3. **Flexibility**: Harder to swap clangd versions on the fly

### Recommendation
**âœ… Recommended** - Benefits outweigh disadvantages for most use cases.

## Question 3: clangd-index-server & Index Files

### What is clangd-index-server?

clangd-index-server is a separate service that:
- Provides pre-built symbol indexes
- Serves index data over network
- Reduces per-client memory usage
- Enables shared indexing across multiple clients

### Use Cases

**Good fit for:**
- Large codebases (>10,000 files)
- Multiple concurrent users
- Cloud/remote deployments
- Limited client memory

**Not needed for:**
- Small/medium projects (<10,000 files)
- Single-user scenarios
- Fast local SSDs
- Sufficient memory available

### Memory/Performance Trade-offs

#### Without index-server (current implementation):
- **Memory**: 500MB - 4GB per instance (depends on project size)
- **Startup**: 10s - 5min (first index build)
- **Performance**: Fast after initial indexing
- **Scalability**: Limited by memory

#### With index-server:
- **Memory**: 200MB - 1GB per client (reduced)
- **Index-server memory**: 2GB - 8GB (shared)
- **Startup**: 2s - 10s (using pre-built index)
- **Performance**: Slightly slower due to network
- **Scalability**: Better for multiple clients

### Implementation Complexity

Adding clangd-index-server requires:
1. Additional container for index server
2. Network configuration between services
3. Pre-building index files (can be automated)
4. More complex docker-compose setup
5. Index file storage (volumes)

### Recommendation for index-server

**Not recommended initially** - Add complexity without clear benefit for single-user scenarios.

**Consider adding if:**
- Multiple concurrent users (>3)
- Large codebase (>50,000 files)
- Memory constraints (<2GB available)
- Using Kubernetes/cloud deployment

## Question 4: Memory/Performance Optimizations

### Current Implementation

The implementation includes several optimizations:

1. **Resource Limits** (docker-compose.yml)
   ```yaml
   resources:
     limits:
       memory: 2G
       cpus: '2.0'
   ```

2. **Cache Volumes**
   ```yaml
   volumes:
     - clangd-cache:/root/.cache/clangd
   ```

3. **Health Checks**
   - Monitors server status
   - Enables automatic restarts

### Recommended clangd Flags for Memory Optimization

```bash
# Low memory mode (< 1GB)
--background-index-priority=low
--limit-results=20
--malloc-trim

# Medium memory mode (1-2GB)
--background-index
--limit-results=50
--background-index-priority=normal

# High performance mode (2GB+)
--background-index
--limit-results=100
--background-index-priority=normal
--compile-commands-dir=/path/to/build
```

### Project Size Guidelines

| Project Size | Files | Memory | CPU | Recommended Setup |
|-------------|-------|---------|-----|-------------------|
| Small | <1,000 | 512MB | 0.5 core | Default config |
| Medium | 1K-10K | 1-2GB | 1-2 cores | Add `--limit-results=50` |
| Large | 10K-50K | 2-4GB | 2-4 cores | Use `compile_commands.json` |
| Very Large | >50K | 4GB+ | 4+ cores | Consider index-server |

### Performance Optimizations

1. **Use compile_commands.json**
   - Precise indexing (only compiled files)
   - Faster startup
   - Lower memory usage
   - Generate with: `cmake -DCMAKE_EXPORT_COMPILE_COMMANDS=ON`

2. **Persistent cache**
   - Reuse index across restarts
   - Faster subsequent startups
   - Already implemented in docker-compose.yml

3. **Background indexing**
   - Index while working
   - Lower priority to avoid blocking
   - Use `--background-index-priority=low`

4. **Limit results**
   - Reduce memory for large result sets
   - Use `--limit-results=N`
   - Trade-off: might miss some results

5. **Project-specific settings**
   - Use `.clangd` config file in project root
   - Customize per-project settings
   - Example:
     ```yaml
     CompileFlags:
       Add: [-std=c++17]
     Index:
       Background: Build
     ```

### Memory Monitoring

Add to docker-compose.yml:
```yaml
logging:
  driver: "json-file"
  options:
    max-size: "10m"
    max-file: "3"
```

Monitor with:
```bash
docker stats mcp-language-server
```

### Benchmark Results (Estimated)

Based on typical configurations:

| Configuration | Startup | Memory | CPU | Response Time |
|--------------|---------|---------|-----|---------------|
| stdio (local) | 30s | 1.5GB | 50% | 50ms |
| HTTP (local) | 32s | 1.6GB | 52% | 55ms |
| HTTP (remote) | 35s | 1.6GB | 52% | 150ms |
| With index-server | 5s | 800MB | 30% | 80ms |

## Final Recommendations

### Immediate Actions (Already Implemented)
âœ… 1. HTTP/SSE transport support
âœ… 2. Docker container with clangd
âœ… 3. Basic resource limits
âœ… 4. Health checks
âœ… 5. Cache volumes

### Next Steps (If Needed)
ðŸ“‹ 1. Add memory monitoring and alerts
ðŸ“‹ 2. Create project-size-specific configurations
ðŸ“‹ 3. Add Prometheus metrics endpoint
ðŸ“‹ 4. Document common memory issues and solutions

### Future Enhancements (Low Priority)
ðŸ”® 1. clangd-index-server support (for very large projects)
ðŸ”® 2. Pre-built index images (for common projects)
ðŸ”® 3. Auto-scaling based on load
ðŸ”® 4. Multi-language server support

## Conclusion

The implementation provides significant improvements:

1. **HTTP streaming**: Better for containers, monitoring, and scaling
2. **Docker packaging**: Simplified deployment with clangd included
3. **Resource management**: Configurable limits and optimization options
4. **Production-ready**: Health checks, caching, and documentation

For memory/performance concerns:
- Start with default configuration (2GB, 2 cores)
- Add `--limit-results=50` for large projects
- Use `compile_commands.json` for precise indexing
- Consider index-server only for very large projects (>50K files) with multiple users

The current implementation strikes a good balance between simplicity, performance, and resource usage.
